{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f5d604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1136c13a0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from tavily import TavilyClient\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature = 0)\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "\n",
    "os.environ[\"TIVILY_API_KEY\"] = \"\"\n",
    "\n",
    "### State\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    documents: List[str]\n",
    "    relevant_documents: List[str]\n",
    "    source_info : str\n",
    "    TavilyChecked: bool\n",
    "    HallucinationChecked: bool\n",
    "    answer: str\n",
    "\n",
    "\n",
    "### Nodes\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "    # Retrieval\n",
    "    urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "    ]\n",
    "\n",
    "    docs = [WebBaseLoader(url).load() for url in urls]\n",
    "    docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=250, chunk_overlap=0\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "    # Add to vectorDB\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=doc_splits,\n",
    "        collection_name=\"rag-chroma\",\n",
    "        embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    documents = retriever.invoke(question)\n",
    "    print(question)\n",
    "    print(documents)\n",
    "    return {\"documents\": documents, \"relevant_documents\": [], \"question\": question, \"TavilyChecked\": False, \"HallucinationChecked\": False}\n",
    "\n",
    "def relevance_checker(state):\n",
    "    print(\"---RELEVANCE CHECKER---\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    system = \"\"\"You are a grader assessing relevance\n",
    "        of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
    "        grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "        Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "        \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"question: {question}\\ndocument: {retrieved_chunk}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    question_router = prompt | llm | JsonOutputParser()\n",
    "    query = state[\"question\"]\n",
    "\n",
    "    docs = state[\"documents\"]\n",
    "    relevant_docs = []\n",
    "    for doc in docs:\n",
    "        chunk = doc.page_content\n",
    "\n",
    "        result = question_router.invoke({\n",
    "            \"question\": query,\n",
    "            \"retrieved_chunk\": chunk\n",
    "            })\n",
    "\n",
    "        if result.get(\"score\") == \"yes\":\n",
    "            relevant_docs.append(doc)\n",
    "\n",
    "    if len(relevant_docs) == 0:\n",
    "        if state[\"TavilyChecked\"]:\n",
    "            print(\"---TAVILY CHECKED---\")\n",
    "            print(\"failed: not relevant\")\n",
    "            return \"Failed\"\n",
    "        else:\n",
    "            print(\"---NOT RELEVANT. TAVILY SEARCH NEEDED---\")\n",
    "            return \"No\"\n",
    "    else:\n",
    "            state[\"relevant_documents\"] = relevant_docs\n",
    "            return \"Yes\"\n",
    "\n",
    "def tavily_searcher(state):\n",
    "    \"\"\"\n",
    "    Search Tavily based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "    print(\"---TAVILY SEARCH---\")\n",
    "\n",
    "    tavily = TavilyClient(api_key='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')\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    response = tavily.search(query=question, max_results=5)\n",
    "    documents = [obj[\"content\"] for obj in response[\"results\"]]\n",
    "\n",
    "    return {\n",
    "        \"documents\": documents, \"TavilyChecked\": True\n",
    "    }\n",
    "\n",
    "def generate_answer(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"relevant_documents\"]\n",
    "\n",
    "    system = \"\"\"You are an assistant for question-answering tasks.\n",
    "        Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "        Use three sentences maximum and keep the answer concise\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"question: {question}\\n\\n context: {context} \"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "    answer = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "\n",
    "    return {\n",
    "        \"relevant_documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer \n",
    "    }\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "\n",
    "### Conditional edge\n",
    "\n",
    "\n",
    "def hallucination_checker(state):\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"relevant_documents\"]\n",
    "    answer = state[\"answer\"]  # ✅ 추가\n",
    "\n",
    "    system = \"\"\"You are a grader assessing whether\n",
    "    an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate\n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a\n",
    "    single key 'score' and no preamble or explanation.\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"documents: {documents}\\n\\nanswer: {answer}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"answer\": answer}\n",
    "    )\n",
    "    grade = score[\"score\"]\n",
    "\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        state[\"HallucinationChecked\"] = True\n",
    "        return \"Yes\"\n",
    "    else:\n",
    "        if state[\"HallucinationChecked\"]:\n",
    "            print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "            state[\"HallucinationChecked\"] = True\n",
    "            return \"No\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, ABORT---\")\n",
    "            return \"Failed\"\n",
    "\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"docs_retrieval_node\", retrieve)\n",
    "workflow.add_node(\"search_tavily_node\", tavily_searcher)\n",
    "workflow.add_node(\"generate_answer_node\", generate_answer)  # generatae\n",
    "workflow.add_node(\"hallucination_checker_node\", hallucination_checker)  # generatae\n",
    "\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\n",
    "    \"docs_retrieval_node\"\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"docs_retrieval_node\",\n",
    "    relevance_checker,\n",
    "    {\n",
    "        \"No\": \"search_tavily_node\",\n",
    "        \"Yes\": \"generate_answer_node\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"search_tavily_node\",\n",
    "    relevance_checker,\n",
    "    {\n",
    "        \"Failed\": END,\n",
    "        \"Yes\": \"generate_answer_node\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_answer_node\",\n",
    "    hallucination_checker,\n",
    "    {\n",
    "        \"Yes\": END,\n",
    "        \"No\": \"generate_answer_node\",\n",
    "        \"Failed\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "\n",
    "inputs = {\"question\": \"What is prompt?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
